{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPW6rzj2y0MeV+bbJc0Zhq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# adapted from bertviz tutorial (https://github.com/jessevig/bertviz)\n",
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not torch.is_grad_enabled():\n",
        "      return\n",
        "\n",
        "    parsed = einops.parse_shape(tensor, names)\n",
        "\n",
        "    for name, new_dim in parsed.items():\n",
        "      old_dim = self.shapes.get(name, None)\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\\")"
      ],
      "metadata": {
        "id": "QQ1Ua9dB9_HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "\n",
        "embed = nn.Embedding(target_text_processor.vocabulary_size(),\n",
        "                     UNITS, padding_idx=0)\n",
        "ex_tar_embed = embed(torch.tensor(ex_tar_in))\n",
        "\n",
        "result, _ = attention_layer(ex_tar_embed, ex_context)\n",
        "\n",
        "print(f'Input Sequence, shape (batch, s, units): {ex_context.shape}')\n",
        "print(f'Target Sequence, shape (batch, t, units): {ex_tar_embed.shape}')\n",
        "print(f'Attention Result, shape (batch, t, units): {result.shape}')\n",
        "print(f'Attention Weights, shape (batch, t, s):    {attention_layer.last_attention_weights.shape}')"
      ],
      "metadata": {
        "id": "yaRS7EfPjfkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_weights = attention_layer.last_attention_weights\n",
        "mask=(ex_context_tok != 0).numpy()\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(mask*attention_weights[:, 0, :])\n",
        "plt.title('Attention Weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(mask)\n",
        "plt.title('Mask');\n"
      ],
      "metadata": {
        "id": "urzpoQHajgbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from bertviz tutorial (https://github.com/jessevig/bertviz)\n",
        "import torch\n",
        "\n",
        "def att_map(self,\n",
        "            texts, *,\n",
        "            max_length=50,\n",
        "            temperature=0.0):\n",
        "    context = self.encoder.convert_input(texts)\n",
        "    batch_size = texts.shape[0]\n",
        "\n",
        "    tokens = []\n",
        "    attention_weights = []\n",
        "    next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        next_token, done, state = self.decoder.get_next_token(\n",
        "            context, next_token, done, state, temperature)\n",
        "\n",
        "        tokens.append(next_token)\n",
        "        attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "        if done.all():\n",
        "            break\n",
        "\n",
        "    tokens = torch.cat(tokens, dim=-1)\n",
        "    self.last_attention_weights = torch.cat(attention_weights, dim=1)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uESvDUu7jkH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adapted from https://github.com/jessevig/bertviz \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def plot_attention(self, text, **kwargs):\n",
        "  assert isinstance(text, str)\n",
        "  output = self.translate([text], **kwargs)\n",
        "  output = output[0].detach().cpu().numpy().decode()\n",
        "\n",
        "  attention = self.last_attention_weights[0]\n",
        "\n",
        "  context = torch.lower_and_split_punct(text)\n",
        "  context = context.numpy().decode().split()\n",
        "\n",
        "  output = torch.lower_and_split_punct(output)\n",
        "  output = output.numpy().decode().split()[1:]\n",
        "\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  ax.matshow(attention, vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + context, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + output, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')"
      ],
      "metadata": {
        "id": "YBzc6xiLj5yi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}