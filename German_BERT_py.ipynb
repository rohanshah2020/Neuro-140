{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJw0rtKqk874R1SlYIPGTY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnzpBA59inX9"
      },
      "outputs": [],
      "source": [
        "# pretrained model taken from https://huggingface.co/bert-base-german-cased\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from overrides import overrides\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer, BertForMaskedLM, BertTokenizer\n",
        "from typing import List\n",
        "\n",
        "from language_models.lm import LanguageModel\n",
        "\n",
        "\n",
        "class MaskedLanguageModelBert(LanguageModel):\n",
        "\n",
        "    def __init__(self, pretrained_weights: str, lm_dir: str):\n",
        "        super(MaskedLanguageModelBert, self).__init__()\n",
        "        self._pretrained_weights: str = pretrained_weights\n",
        "        self._lm_dir: str = lm_dir\n",
        "        self.language_model: BertForMaskedLM = None\n",
        "        self.tokenizer: BertTokenizer = None\n",
        "        self._set_language_model_and_tokenizer(self._pretrained_weights)\n",
        "\n",
        "    def _set_language_model_and_tokenizer(self, pretrained_weights: str) -> None:\n",
        "        self._pretrained_weights = pretrained_weights\n",
        "\n",
        "        try:\n",
        "            self.language_model = BertForMaskedLM.from_pretrained(self._pretrained_weights, cache_dir=self._lm_dir)\n",
        "            self.tokenizer = BertTokenizer.from_pretrained(self._pretrained_weights, cache_dir=self._lm_dir)\n",
        "        except OSError:\n",
        "            self.language_model = AutoModelWithLMHead.from_pretrained(self._pretrained_weights, cache_dir=self._lm_dir)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self._pretrained_weights, cache_dir=self._lm_dir)\n",
        "\n",
        "    @overrides\n",
        "    def get_mask_token(self) -> str:\n",
        "        return self.tokenizer.mask_token\n",
        "\n",
        "    @overrides\n",
        "    def get_mask_id(self) -> torch.int64:\n",
        "        return self.tokenizer.mask_token_id\n",
        "\n",
        "    @overrides\n",
        "    def get_unkown_token(self) -> str:\n",
        "        return self.tokenizer.unk_token\n",
        "\n",
        "    @overrides\n",
        "    def get_unkown_id(self) -> torch.int64:\n",
        "        return self.tokenizer.unk_token_id\n",
        "\n",
        "    @overrides\n",
        "    def tokenize(self, sequence: str) -> List:\n",
        "        return self.tokenizer.encode(sequence, add_special_tokens=True, return_tensors=\"pt\")\n",
        "\n",
        "    @overrides\n",
        "    def get_token(self, idx: torch.Tensor) -> str:\n",
        "        token = self.tokenizer.decode([idx])\n",
        "        return token\n",
        "\n",
        "    @overrides\n",
        "    def get_index(self, token: str) -> int:\n",
        "        idx = self.tokenizer.encode(text=[token], add_special_tokens=False)\n",
        "        return idx[0]\n",
        "\n",
        "    @overrides\n",
        "    def sanitize_tokens(self, tokens: torch.Tensor) -> List[int]:\n",
        "        return tokens[0].tolist()\n",
        "\n",
        "    @overrides\n",
        "    def predict(self, tokens: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
        "        with torch.no_grad():\n",
        "            logits = self.language_model(tokens)[0][0]\n",
        "        probabilities = F.softmax(logits, dim=1).detach()\n",
        "        input_token_ids = tokens[0].detach()\n",
        "        assert (len(probabilities) == len(input_token_ids)), 'Sanity check failed, dimensions do not match.'\n",
        "        return probabilities, input_token_ids\n",
        "\n",
        "        tensor_input = torch.tensor(token_ids_input)#.unsqueeze(0)\n",
        "        predictions = self.language_model(tensor_input)[0]\n",
        "        loss_fct = torch.nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(predictions.squeeze(), tensor_input.squeeze()).data\n",
        "        loss = loss.item()\n",
        "        return loss, tokenized"
      ]
    }
  ]
}